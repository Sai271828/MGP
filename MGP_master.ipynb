{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahIsZjjNp1te"
   },
   "source": [
    "# Math genealogy project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RV2npq46AcA"
   },
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2023\n",
    "Authors: [Venkata Sai Narayana Bavisetty](https://www.linkedin.com/in/venkata-sai-narayana-bavisetty-62814476/), [Karthik Vasu](https://www.linkedin.com/in/karthikvasu/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEDB05nMp9IS"
   },
   "source": [
    "In this notebook, we outline the process of downloading math genealogy data and coordinates for different universities. We will conclude by analyzing the data and providing explanations for some of our findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGXIVaVvqf4h"
   },
   "source": [
    "# Downloading math genealogy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd56xQltq0Nf"
   },
   "source": [
    "In this notebook, we outline the process of downloading Math genealogy data and coordinates for different universities. We will conclude by analyzing the data and providing explanations for some of our findings.\n",
    "\n",
    "We utilized the [MGP API](https://mathgenealogy.org:8000/api/v2/MGP/) for data retrieval. Below is the script code for downloading the data. While the script is primarily sourced from the [MGP website](https://mathgenealogy.org:8000/login), we made modifications to implement exponential backoff in case of request failures. This adaptation is crucial for successfully downloading the entire dataset in one go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp9xEZeR3IV7"
   },
   "source": [
    "\n",
    "We download data in batches of $100$ IDs. Although the website indicates that we can download $10,000$ IDs at once, we encountered difficulties when attempting to do so in batches of $10,000$ or even $1,000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjya_gFbtv8p",
    "outputId": "d075c3fa-0326-4f80-cacf-2fa40ecb5070"
   },
   "outputs": [],
   "source": [
    "#pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4NM6ShGfPSLu"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import backoff\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YzGYypy7qzih"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enter email used for MGP authentication: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vb8@illinois.edu\n",
      "········\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6156\\1809102562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mquerydata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'start'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"stop\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquerydata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[0;32m    340\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[0;32m    341\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'surrogatepass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "PROTOCOL = \"https\"\n",
    "HOSTNAME = \"mathgenealogy.org\"\n",
    "PORT = \"8000\"\n",
    "\n",
    "# Function to prompt on the console for the user's\n",
    "# email address and password to use to log in to\n",
    "# the MGP API. Password is not shown on console\n",
    "# while the user types it.\n",
    "def getlogin():\n",
    "    print(\"Enter email used for MGP authentication:\",end=\" \",file=sys.stderr)\n",
    "    email = input()\n",
    "    password = getpass()\n",
    "    return {'email': email, 'password': password}\n",
    "\n",
    "# Function to log in to the MGP API and get a JWT for authentication.\n",
    "# authdata is a dict with key email set to the user's email address\n",
    "# and key password set to the user's password.\n",
    "# If login is successful, returns a JSON object with key token set\n",
    "# to the JWT.\n",
    "# If login is unsuccessful, raises RuntimeError.\n",
    "def login(authdata):\n",
    "    r = requests.post(f\"{PROTOCOL}://{HOSTNAME}:{PORT}/login\", authdata)\n",
    "    if r.ok:\n",
    "        r.close()\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.close()\n",
    "        raise RuntimeError(\"Failed to authenticate\")\n",
    "\n",
    "# Function to do a query against the MGP API. Returns a string with\n",
    "# the query result if the query was successfully executed. Raises\n",
    "# RuntimeError if there is an error.\n",
    "\n",
    "# endpoint is a string (beginning with /) such as \"/api/v2/MGP/acad/all\"\n",
    "# referring to an API endpoint\n",
    "\n",
    "# token is a dict with key token containing a JWT.\n",
    "# The return value from login() is the best way to get this.\n",
    "\n",
    "# params is a dict structured to contain the GET parameters.\n",
    "# For example, if quering /api/v2/MGP/acad, params could be\n",
    "# {'id': '1969'}.\n",
    "@backoff.on_exception(backoff.expo, (requests.exceptions.Timeout,requests.exceptions.RequestException), max_tries= 8)\n",
    "def doquery(endpoint,token, params):\n",
    "    headers = {'x-access-token': token['token']}\n",
    "    r = requests.get(f\"{PROTOCOL}://{HOSTNAME}:{PORT}{endpoint}\",headers = headers, params = params)\n",
    "    if r.ok:\n",
    "        r.close()\n",
    "        return r.text\n",
    "\n",
    "download_data = True\n",
    "\n",
    "# User modifications can be made here in this portion of the script.\n",
    "# Alternatively, import the functions above into your own python\n",
    "# script and write code similar to what is given here as the example.\n",
    "if download_data:\n",
    "    data=[]\n",
    "\n",
    "    authdata = getlogin()\n",
    "\n",
    "    # Get all ids from MGP\n",
    "    endpoint = '/api/v2/MGP/acad/all'\n",
    "    querydata = ''\n",
    "    token = login(authdata)\n",
    "    all_ids = json.loads(doquery(endpoint,token,querydata))\n",
    "    \n",
    "    # Download data for IDs\n",
    "    # We will download in batches of 100\n",
    "    max_id= int(max(all_ids)/100) +1\n",
    "    endpoint = '/api/v2/MGP/acad/range'\n",
    "    json_file = \"MGP_data_try.json\"\n",
    "\n",
    "    for id in range(max_id):\n",
    "        # The token changes every two hours\n",
    "        token = login(authdata)\n",
    "\n",
    "        querydata = {'start': id*100, \"stop\":(id+1)*100}\n",
    "        new_data = json.loads(doquery(endpoint,token,querydata))\n",
    "        data.extend(new_data)\n",
    "\n",
    "        # Save data to a JSON file\n",
    "        with open(json_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data is formatted as a JSON file. Each row is a dictionary featuring the key `MGP_academic`. The corresponding value for this key is another dictionary encompassing the details of individuals within the math genealogy dataset. A sample element is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3EbVBQeXpodv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file\n",
    "with open('C:\\\\Users\\\\karth\\\\Code\\\\MGP\\\\data\\\\MGP_data.json', 'r') as file:\n",
    "    all_data = json.load(file)\n",
    "print(all_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we access the JSON file and convert it into a Pandas dataframe with the required information. The data frame has the columns `ID`, `family_name`,`given_name`,`school`,`year`,`descendants`,`country` and `subject`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bNW3E-U90-H5"
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "school_split_len = []\n",
    "for temp_row in all_data:\n",
    "    row = temp_row['MGP_academic']\n",
    "\n",
    "    try:\n",
    "        school = row['student_data']['degrees'][0]['schools'][0]\n",
    "    except:\n",
    "        school = 'None'\n",
    "\n",
    "    split = school.split(',')\n",
    "    school_split_len.append(len(split))\n",
    "    if len(split)>1:\n",
    "        country = split[-1]\n",
    "    school = split[0]\n",
    "\n",
    "    try:\n",
    "        year = int(row['student_data']['degrees'][0]['degree_year'])\n",
    "    except:\n",
    "        year = 'None'\n",
    "    \n",
    "    try:\n",
    "        subject = int(row['student_data']['degrees'][0]['degree_msc'])\n",
    "    except:\n",
    "        subject = 'None'\n",
    "\n",
    "\n",
    "\n",
    "    new_row = {\n",
    "        'ID':row['ID'],\n",
    "        'family_name':row['family_name'],\n",
    "        'given_name':row['given_name'],\n",
    "        'school':school,\n",
    "        'year':year,\n",
    "        'descendants':row['student_data']['descendants']['descendant_count'],\n",
    "        'country':country,\n",
    "        'subject':subject\n",
    "        }\n",
    "    data.append(new_row)\n",
    "\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWCFqDsK0-X2"
   },
   "source": [
    "# Downloading the coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FcRO626V09fk"
   },
   "outputs": [],
   "source": [
    "def get_coordinates(university_name,use_Bing=False,bing_api_key=None):\n",
    "\n",
    "    # Bing API geocoder\n",
    "    if use_Bing:\n",
    "        geolocator = Bing(api_key=bing_api_key)\n",
    "\n",
    "    # Nominatim geocoder\n",
    "    else:\n",
    "        geolocator = Nominatim(user_agent=\"university_locator\")\n",
    "\n",
    "    university_name_no_filler = remove_fillers(university_name)\n",
    "\n",
    "    try:\n",
    "        # Use the geocode method to get the location information\n",
    "        location = geolocator.geocode(university_name_no_filler)\n",
    "\n",
    "        if location:\n",
    "            # Extract latitude and longitude\n",
    "            latitude, longitude = location.latitude, location.longitude\n",
    "            return [university_name,latitude,longitude]\n",
    "\n",
    "        else:\n",
    "            # if we did not get the location return location in Antartica\n",
    "            return [university_name,-82,135]\n",
    "            #print(f\"Coordinates for {university_name} not found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(f\"An error occurred: {e}\")\n",
    "        #If there is an error we return None\n",
    "        return [university_name,None,None]\n",
    "\n",
    "def remove_fillers(sentence):\n",
    "\n",
    "    sentence = str(sentence)\n",
    "    # List of filler words\n",
    "    filler_words = ['the', 'of', 'and', 'in', 'to', 'a', 'is', 'that', 'it', 'with', 'as', 'on', 'for', 'at']\n",
    "\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Remove filler words\n",
    "    filtered_words = [word for word in words if word.lower() not in filler_words]\n",
    "\n",
    "    # Join the remaining words to form the new sentence\n",
    "    new_sentence = ' '.join(filtered_words)\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "BqEQH7bROyux"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set download_coordinates to True if you want to download the coordinates\n",
    "download_coordinates = False\n",
    "\n",
    "# If you want to use Bing maps API, set use_Bing to True and enter your API key\n",
    "use_Bing = False\n",
    "bing_api_key = None\n",
    "\n",
    "if download_coordinates:\n",
    "\n",
    "    import time\n",
    "\n",
    "    # Using Bing maps API for better performance. This requires a free account and an API key\n",
    "    from geopy.geocoders import Bing\n",
    "\n",
    "    # Alternatively we can use OpenStreetMaps API which is free and open source, but does not perform as well as Bing\n",
    "    from geopy.geocoders import Nominatim\n",
    "\n",
    "    # Get the unique schools\n",
    "    schools = data['school'].unique()\n",
    "\n",
    "    # Loop through the schools and save the recieved coordinates in coord\n",
    "    coords =[]\n",
    "    count = 0\n",
    "    for school in tqdm(schools):\n",
    "        coord = get_coordinates(school,use_Bing=use_Bing,bing_api_key=bing_api_key)\n",
    "        coords.append(coord)\n",
    "        count+=1\n",
    "\n",
    "        # Save coordinates every 500 iterations\n",
    "        if count%500==0:\n",
    "            with open('data/coords'+str(count)+'.csv','w', newline='') as file:\n",
    "                csv_writer = csv.writer(file)\n",
    "                csv_writer.writerows(coords)\n",
    "\n",
    "        # To prevent the API from blocking us\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Save all the coordinates\n",
    "    with open('data/coordinates.csv','w', newline='') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerow(['school','lat','lon'])\n",
    "        csv_writer.writerows(coords)\n",
    "    print(\"Done Downloading Coordinates\")\n",
    "\n",
    "else:\n",
    "    # Read the coordinates from the csv file\n",
    "    coords = pd.read_csv('C:\\\\Users\\\\karth\\\\Code\\\\MGP\\\\data\\\\coordinates.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the coordinates with the data\n",
    "data = pd.merge(data,coords,on='school',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz0ZZ7Qb1Lt1"
   },
   "source": [
    "# Making heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "w8aIs9yHp0Ua"
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "import folium\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inf_univ(full_data, t0 = 1950, t1 = 1960, ignore_ones = False, show_map = True):\n",
    "    period = full_data[ (full_data[\"year\"]>=t0) & (full_data['year']<t1)]\n",
    "\n",
    "    p = period[[\"school\",\"lat\",\"lon\",\"descendants\"]].groupby(['school'])\n",
    "    p = p[\"descendants\"].agg(['sum']).reset_index()\n",
    "    p = pd.merge(p,coords,on='school',how='left')\n",
    "\n",
    "    X = p[[\"lat\",\"lon\",\"sum\"]]\n",
    "    X.dropna(inplace=True)\n",
    "    if ignore_ones:\n",
    "        X = X[X[\"sum\"] != 1]\n",
    "        map = folium.Map(location = [5,30], tiles = \"Cartodb dark_matter\", zoom_start=2)\n",
    "\n",
    "        heat_data = X.values.tolist()\n",
    "\n",
    "        plugins.HeatMap(heat_data).add_to(map)\n",
    "\n",
    "        map.save(f\"heat_map_ignore_{t0}_to_{t1}.html\")\n",
    "        if show_map:\n",
    "            display(map)\n",
    "        return map\n",
    "        \n",
    "    else:\n",
    "        map = folium.Map(location = [5,30], tiles = \"Cartodb dark_matter\", zoom_start=2)\n",
    "\n",
    "        heat_data = X.values.tolist()\n",
    "\n",
    "        plugins.HeatMap(heat_data).add_to(map)\n",
    "\n",
    "        map.save(f\"heat_map_{t0}_to_{t1}.html\")\n",
    "        if show_map:\n",
    "            display(map)\n",
    "        return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\karth\\Code\\MGP\\MGP\\MGP_master.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karth/Code/MGP/MGP/MGP_master.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mmap\u001b[39m \u001b[39m=\u001b[39m plot_inf_univ(data, ignore_ones\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\karth\\Code\\MGP\\MGP\\MGP_master.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karth/Code/MGP/MGP/MGP_master.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_inf_univ\u001b[39m(full_data, t0 \u001b[39m=\u001b[39m \u001b[39m1950\u001b[39m, t1 \u001b[39m=\u001b[39m \u001b[39m1960\u001b[39m, ignore_ones \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, show_map \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karth/Code/MGP/MGP/MGP_master.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     period \u001b[39m=\u001b[39m full_data[ (full_data[\u001b[39m\"\u001b[39m\u001b[39myear\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39mt0) \u001b[39m&\u001b[39m (full_data[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m<\u001b[39mt1)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karth/Code/MGP/MGP/MGP_master.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     p \u001b[39m=\u001b[39m period[[\u001b[39m\"\u001b[39m\u001b[39mschool\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlon\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mdescendants\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mschool\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karth/Code/MGP/MGP/MGP_master.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     p \u001b[39m=\u001b[39m p[\u001b[39m\"\u001b[39m\u001b[39mdescendants\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39magg([\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__ge__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ge__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cmp_method(other, operator\u001b[39m.\u001b[39mge)\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6096\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6093\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   6095\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 6096\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6098\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:293\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    292\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:82\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[0;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mscalar_compare(x\u001b[39m.\u001b[39mravel(), y, op)\n\u001b[0;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\karth\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\ops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "map = plot_inf_univ(data, ignore_ones= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
